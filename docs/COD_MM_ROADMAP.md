# COD Matchmaking Whitepaper Implementation Roadmap

## Overview

This roadmap guides the implementation of a full agent-based matchmaking simulation that matches the mathematical model described in `cod_matchmaking_model.md`. The current codebase already implements a substantial portion of the whitepaper (approximately Stage 1-2), and this document breaks down the remaining work into **vertical slices** that can be implemented incrementally.

**üìù Important**: See `NOTES.md` for implementation learnings, common mistakes, and gotchas discovered during development.

### Progress Summary

**Completed Slices**:
- ‚úÖ **Slice A: Parties & Multi-Player Search Objects**
  - Full party system integrated into matchmaking
  - WASM frontend integration complete
  - Party metrics and visualizations added
  - Automatic party generation via config parameter
- ‚úÖ **Slice B: Matchmaking Constraints & Backoff Refinement**
  - Fixed critical units mismatch bug (ticks ‚Üí seconds)
  - Fixed skill range check to match whitepaper ¬ß3.3 exactly
  - Added optional debug logging behind feature flag
  - All backoff formulas verified correct
- ‚úÖ **Slice C: Team Balancing & Blowout Modeling**
  - Exact team balancing for small playlists (6v6) using Karmarkar-Karp partitioning
  - Enhanced blowout detection with severity classification (Mild, Moderate, Severe)
  - Configurable win probability logistic with gamma parameter
  - Per-playlist blowout rate tracking
  - Team skill difference distribution tracking
  - Frontend charts for blowout metrics and severity distribution
- ‚úÖ **Slice D: Performance Model & Skill Evolution**
  - Per-match performance model with configurable noise
  - Skill update rule: \(s_i^+ = s_i^- + \alpha(\hat{y}_i - \mathbb{E}[Y_i])\)
  - Batch-based percentile recalculation
  - Skill distribution evolution tracking over time
  - Frontend visualizations: skill evolution time series, current skill distribution, performance distribution
  - Toggle between static and evolving skill modes
  - Skill drift summary metrics
- ‚úÖ **Slice E: Satisfaction, Continuation, and Retention Modeling**
  - Formal retention model with logistic function and experience vectors
  - Return probability model (between-sessions)
  - Effective population size and churn tracking
  - Population change rate metric
  - Retention presets (ping-first, skill-first, lenient, strict)
  - Diagnostic panel for retention model debugging
- ‚úÖ **Slice F: Region/DC Graph & Regional Metrics**
  - Explicit Region enum (NorthAmerica, Europe, AsiaPacific, SouthAmerica, Other)
  - Region adjacency graph with realistic geographic connections
  - Region-aware backoff (best region ‚Üí adjacent ‚Üí all based on wait time)
  - Per-region configuration overrides (max ping, delta ping, skill similarity)
  - Regional metrics tracking (search time, delta ping, blowout rate, cross-region match rate)
  - Frontend region-split charts and region filter dropdown
  - Per-region config UI panel
- ‚úÖ **Slice G: Frontend Experiment Runner & Visualizations**
  - Comprehensive experiment storage system with localStorage persistence
  - Enhanced experiment runner supporting single and multi-parameter sweeps
  - Scenario preset system with built-in presets (SBMM, retention, regional, party, evolution)
  - Experiment library with search, filtering, tags, and CRUD operations
  - Side-by-side experiment comparison tool (2-4 experiments)
  - Export/import experiments as JSON files
  - Progress tracking with non-blocking execution (optimized to prevent UI freezing)
  - Reusable chart components (MetricChart, ComparisonChart, HeatmapChart)
  - Experiment builder UI for visual configuration
  - All metrics from slices A-F accessible via experiment runner

**Remaining Slices**: H (optional)

### Relationship to Whitepaper

The whitepaper (`cod_matchmaking_model.md`) describes:
- **Section 2**: State & variables (players, DCs, playlists, skill, search objects)
- **Section 3**: High-fidelity matchmaking process (seed+greedy, feasibility, quality scoring, team balancing)
- **Section 4-5**: Reduced/aggregate models for scale (optional, later phase)
- **Section 6**: Treatment of each CoD variable (connection, skill, input, platform, etc.)
- **Section 7**: Concrete build order (Stage 0-4)

This roadmap focuses on completing the **agent-based model** (Stages 1-3) and preparing for the aggregate model (Stage 4, optional).

### Current Implementation Status

The Rust/WASM engine (`src/`) already implements:
- ‚úÖ Player state machine (OFFLINE ‚Üí IN_LOBBY ‚Üí SEARCHING ‚Üí IN_MATCH)
- ‚úÖ Data centers with ping modeling and backoff
- ‚úÖ Skill system (raw skill, percentiles, buckets)
- ‚úÖ Search objects and seed+greedy matchmaking
- ‚úÖ Feasibility constraints (playlist, size, skill similarity/disparity, DC intersection, server capacity)
- ‚úÖ Quality scoring (ping, skill balance, wait time)
- ‚úÖ Team balancing (exact partitioning for 6v6, snake draft for large playlists)
- ‚úÖ Match outcomes with configurable logistic and blowout severity classification
- ‚úÖ Performance model with per-match performance indices
- ‚úÖ Skill evolution system with update rule and batch percentile recalculation
- ‚úÖ Skill distribution evolution tracking over time
- ‚úÖ Formal retention model with logistic function and experience vectors
- ‚úÖ Return probability model (between-sessions)
- ‚úÖ Population health tracking (effective population size, population change rate)
- ‚úÖ Per-bucket statistics
- ‚úÖ Blowout severity tracking and per-playlist metrics
- ‚úÖ Region adjacency graph and region-aware backoff
- ‚úÖ Regional metrics tracking (per-region search times, delta ping, blowout rates, cross-region match rate)

The React frontend (`web/src/`) provides:
- ‚úÖ Real-time visualization (charts, histograms, bucket stats)
- ‚úÖ Parameter sweeps and experiments
- ‚úÖ Configuration controls
- ‚úÖ Full WASM integration (Rust simulation engine running in browser)
- ‚úÖ Party metrics and visualizations
- ‚úÖ Blowout rate by playlist and severity distribution charts
- ‚úÖ Team balancing configuration controls
- ‚úÖ Skill evolution visualizations (time series, current distribution, drift metrics)
- ‚úÖ Performance distribution charts
- ‚úÖ Skill evolution toggle and configuration controls
- ‚úÖ Retention model diagnostic panel with computed probabilities and config values
- ‚úÖ Population change rate metric (tracks rate of change of effective population)
- ‚úÖ Region-split charts (search time, delta ping, blowout rate by region)
- ‚úÖ Region filter dropdown and per-region configuration UI
- ‚úÖ Cross-region match rate tracking and visualization
- ‚úÖ Comprehensive experiment management system (storage, library, comparison)
- ‚úÖ Enhanced experiment runner with single/multi-param sweeps and progress tracking
- ‚úÖ Scenario preset system with built-in presets for all major configuration categories
- ‚úÖ Experiment export/import functionality
- ‚úÖ Non-blocking experiment execution (optimized to prevent UI freezing)

---

## Current State vs Whitepaper Mapping

| Topic | Whitepaper Section | Current Implementation | Gap / To-Do |
|-------|-------------------|------------------------|-------------|
| **Player State Machine** | ¬ß2.5 | `PlayerState` enum, 4-state loop | ‚úÖ Complete |
| **Player Attributes** | ¬ß2.2 | `Player` struct (location, platform, input, skill, playlists) | ‚úÖ Complete |
| **DC & Ping Model** | ¬ß2.3 | `DataCenter`, `dc_pings`, `acceptable_dcs()` with backoff | ‚úÖ Complete |
| **Skill System** | ¬ß2.4 | Raw skill, percentile, buckets, skill evolution | ‚úÖ Complete |
| **Search Objects** | ¬ß2.7 | `SearchObject` struct | ‚úÖ Complete (supports solo and parties) |
| **Distance Metric** | ¬ß3.1 | `calculate_distance()` with weights | ‚úÖ Complete |
| **Feasibility Checks** | ¬ß3.3 | `check_feasibility()` implements 6 constraints | ‚úÖ Complete (units fixed, skill range check corrected) |
| **Quality Score** | ¬ß3.4 | `calculate_quality()` with 3 components | ‚úÖ Complete |
| **Team Balancing** | ¬ß3.6 | `balance_teams()` with exact partitioning for 6v6 | ‚úÖ Complete (exact for small, snake draft for large) |
| **Match Outcomes** | ¬ß3.7 | `determine_outcome()` with configurable logistic and blowout severity | ‚úÖ Complete (includes performance model) |
| **Skill Evolution** | ¬ß3.7 | Performance model and skill update rule implemented | ‚úÖ Complete |
| **Retention Model** | ¬ß3.8 | Formal logistic model with experience vectors, return probability | ‚úÖ Complete |
| **Parties** | ¬ß2.4, ¬ß2.7 | `Party` struct with full integration | ‚úÖ Complete |
| **Region Graph** | ¬ß2.3, ¬ß6.1 | `Region` enum with adjacency graph, region-aware backoff | ‚úÖ Complete |
| **Under-full Lobbies** | ¬ß6.8 | Exact size match only | ‚ö†Ô∏è Missing |
| **Aggregate Model** | ¬ß5 | None | ‚ùå Optional Phase |

**Legend**: ‚úÖ Complete | ‚ö†Ô∏è Partial/Needs Refinement | ‚ùå Missing

---

## Vertical Slices

Each vertical slice is a self-contained feature that touches engine, metrics, and optionally frontend. Slices can be implemented independently, but some have dependencies (noted below).

### Slice A: Parties & Multi-Player Search Objects ‚úÖ **COMPLETE**

**Whitepaper References**: ¬ß2.4 (party aggregates), ¬ß2.7 (search objects), ¬ß3.6 (team balancing with parties)

**Status**: ‚úÖ **Completed**

**Goals**:
- ‚úÖ Enable players to form parties and search together
- ‚úÖ Build `SearchObject`s from parties (not just solo players)
- ‚úÖ Maintain party integrity during matchmaking (no splitting parties across teams)
- ‚úÖ Compute party-level skill aggregates (\(\bar{s}_P\), \(\Delta s_P\), \(\bar{\pi}_P\), \(\Delta\pi_P\))

**Engine Work**:
- ‚úÖ **`src/types.rs`**:
  - Extended `Party` struct: added `preferred_playlists: HashSet<Playlist>`, `platforms: HashMap<Platform, usize>`, `input_devices: HashMap<InputDevice, usize>`, `avg_location: Location`, `avg_skill_percentile`, `skill_percentile_disparity`
  - Added methods: `Party::from_players(players: &[&Player]) -> Party`, `Party::update_aggregates()`, `Party::to_search_object()`
- ‚úÖ **`src/simulation.rs`**:
  - Added `parties: HashMap<usize, Party>` to `Simulation`
  - Implemented `create_party(player_ids: Vec<usize>) -> Result<usize, String>`
  - Implemented `join_party(party_id: usize, player_id: usize)`, `leave_party(party_id: usize, player_id: usize)`, `disband_party(party_id: usize)`
  - Modified `start_search()`: if player has `party_id`, create `SearchObject` from party; otherwise solo
  - Updated `SearchObject` creation to compute aggregates from party members
  - Added automatic party generation in `generate_population()` based on `party_player_fraction` config
- ‚úÖ **`src/matchmaker.rs`**:
  - Updated `balance_teams()` to respect party boundaries (no splitting parties)
  - Team balancing uses party-aggregated skills when assigning teams
- ‚úÖ **`src/lib.rs`**:
  - Exposed party management methods via WASM bindings
  - Added `get_parties()`, `get_party_members()`, `get_lobby_players()` for UI integration

**Frontend Work**:
- ‚úÖ Full WASM integration replacing JavaScript simulation engine
- ‚úÖ Party metrics displayed in Overview tab (party count, avg size, match rates, search times)
- ‚úÖ Search queue visualization showing solo vs party searches
- ‚úÖ Party size distribution and party vs solo search time comparison charts
- ‚úÖ Config parameter: `party_player_fraction` (0.0-1.0) to control automatic party generation
- ‚úÖ Removed manual party creation UI in favor of automatic generation based on config

**Metrics & Experiments**:
- ‚úÖ Track: average party size, party match rate vs solo match rate, skill disparity within parties
- ‚úÖ Party search times vs solo search times tracked and displayed
- ‚úÖ Party size distribution visualization

**Enhancements Beyond Original Plan**:
- Added `party_player_fraction` config parameter (default 0.5) to automatically generate parties during population creation
- This allows controlled solo vs party mix without manual intervention, aligning with whitepaper's "50% parties of size 2-4" experiment scenario

**Dependencies**: None (was first slice)

---

### Slice B: Matchmaking Constraints & Backoff Refinement

**Whitepaper References**: ¬ß2.3 (DC backoff), ¬ß2.7 (skill backoff), ¬ß3.3 (feasibility)

**Status**: ‚úÖ **COMPLETE**

**Critical Issues Fixed**:
1. ‚úÖ **Units Mismatch (BUG FIXED)**: `SearchObject::wait_time()` now returns seconds by accepting `tick_interval` parameter. All 8 call sites updated.
2. ‚úÖ **Skill Range Check (BUG FIXED)**: Replaced incorrect implementation with correct whitepaper ¬ß3.3 formula: \([\pi_{\min}(M), \pi_{\max}(M)] \subseteq [\ell_j(t), u_j(t)]\) for all searches j.
3. ‚úÖ **Backoff Formulas**: Verified correct - formulas match whitepaper exactly.

**Goals**:
- ‚úÖ Fix units mismatch (ticks ‚Üí seconds) in `wait_time()` and all backoff calls
- ‚úÖ Fix skill range check to match whitepaper ¬ß3.3 exactly
- ‚úÖ Add debug logging for feasibility failures (optional, behind feature flag)
- **Defer**: Under-full lobby support (optional, per whitepaper ¬ß6.8 - only needed for extremely sparse populations)

**Engine Work**:
- ‚úÖ **`src/types.rs`**:
  - ‚úÖ Backoff methods already match whitepaper formulas (no changes needed):
    - \(f_{\text{conn}}(w) = \min(\delta_{\text{init}} + \delta_{\text{rate}} \cdot w, \delta_{\text{max}})\)
    - \(f_{\text{skill}}(w) = \min(\sigma_{\text{init}} + \sigma_{\text{rate}} \cdot w, \sigma_{\text{max}})\)
  - ‚úÖ Fixed `SearchObject::wait_time()`: now returns **seconds** by multiplying ticks by `tick_interval` parameter
- ‚úÖ **`src/matchmaker.rs`**:
  - ‚úÖ Updated all 8 `wait_time()` call sites to pass `tick_interval` parameter
  - ‚úÖ Fixed `check_feasibility()` skill similarity check:
    - For each search j, compute \(\ell_j(t) = \bar{\pi}_j - f_{\text{skill}}(w_j)\) and \(u_j(t) = \bar{\pi}_j + f_{\text{skill}}(w_j)\)
    - Verify \([\pi_{\min}(M), \pi_{\max}(M)] \subseteq [\ell_j(t), u_j(t)]\) for all j
    - Replaced incorrect `skill_range > allowed_range * 2.0` check
  - ‚úÖ Added debug logging (behind `#[cfg(feature = "debug")]`) that records why feasibility checks fail
  - ‚úÖ Fixed skill disparity check to use correct variable name
- ‚úÖ **`src/simulation.rs`**:
  - ‚úÖ Verified `tick_interval` is accessible where `wait_time()` is called
  - ‚úÖ Audited all tick ‚Üî seconds conversions for consistency (all correct)
- ‚úÖ **`src/lib.rs`**:
  - ‚úÖ Updated WASM binding to use new `wait_time()` signature
- ‚úÖ **`Cargo.toml`**:
  - ‚úÖ Added optional `debug` feature flag

**Frontend Work**:
- ‚ö†Ô∏è Debug panel deferred (debug logging available in console when feature enabled)

**Metrics & Experiments**:
- ‚úÖ Added unit tests: `test_wait_time_converts_ticks_to_seconds`, `test_backoff_formulas`, `test_backoff_with_seconds`, `test_skill_range_check_correct`
- ‚úÖ Validated: backoff curves match expected formulas when wait_time is in seconds
- ‚úÖ Validated: skill range constraints work correctly with fixed implementation

**Optional/Future Work** (defer to later slice if needed):
- Under-full lobby support: Add config `allow_underfull_lobbies: bool`, `underfull_threshold: f64`, `underfull_min_wait_seconds: f64`
- Frontend config sliders for under-full lobby parameters
- Frontend debug panel to display feasibility failure reasons

**Dependencies**: None (can be parallel with Slice A)

---

### Slice C: Team Balancing & Blowout Modeling ‚úÖ **COMPLETE**

**Whitepaper References**: ¬ß3.6 (team balancing), ¬ß3.7 (outcomes, blowouts)

**Status**: ‚úÖ **Completed**

**Goals**:
- ‚úÖ Improve team balancing to better approximate Karmarkar-Karp partitioning
- ‚úÖ Enhance blowout detection with more nuanced metrics
- ‚úÖ Track blowout severity/severity buckets

**Engine Work**:
- ‚úÖ **`src/types.rs`**:
  - Added `BlowoutSeverity` enum: `{ Mild, Moderate, Severe }`
  - Extended `Match`: `expected_score_differential: f64`, `win_probability_imbalance: f64`, `blowout_severity: Option<BlowoutSeverity>`
  - Extended `MatchmakingConfig`: `use_exact_team_balancing: bool`, `gamma: f64`, blowout detection coefficients and thresholds
  - Extended `SimulationStats`: `blowout_severity_counts`, `per_playlist_blowout_rate`, `team_skill_difference_samples`, per-playlist tracking fields
- ‚úÖ **`src/matchmaker.rs`**:
  - Refactored `balance_teams()`:
    - Implemented exact partitioning for small playlists (6v6) using recursive backtracking
    - Minimizes `|sum(skills_team1) - sum(skills_team2)|` while respecting party boundaries
    - Falls back to snake draft for large playlists or if exact partitioning fails
    - Always ensures parties stay intact (no splitting)
  - Added `exact_partition_teams()` and `exact_partition_recursive()` helper methods
- ‚úÖ **`src/simulation.rs`**:
  - Enhanced `determine_outcome()`:
    - Uses configurable logistic: \(P(A \text{ wins}) = \sigma(\gamma (S_A - S_B))\) with configurable \(\gamma\)
    - Computes `win_probability_imbalance` and `expected_score_differential`
    - Refactored blowout detection using configurable coefficients for skill difference vs win-probability imbalance
    - Assigns `blowout_severity` based on configurable thresholds
  - Updated `create_matches()` to calculate and store new match fields
  - Updated `process_match_completions()` to track blowout severity and per-playlist stats
  - Updated `update_stats()` to calculate per-playlist blowout rates

**Frontend Work**:
- ‚úÖ Added config sliders:
  - `useExactTeamBalancing` (checkbox)
  - `gamma` (0.5-5.0)
  - Blowout coefficients and thresholds (6 sliders)
- ‚úÖ Added charts:
  - Blowout Rate by Playlist (bar chart)
  - Blowout Severity Distribution (bar chart with color coding)
  - Team Skill Difference Distribution (histogram)
- ‚úÖ Updated `defaultConfig` and `convertConfigToRust()` with new fields
- ‚úÖ Updated stats parsing to include new metrics

**Metrics & Experiments**:
- ‚úÖ Track: team skill difference distribution, blowout rate by playlist, blowout severity breakdown
- ‚úÖ Experiment ready: Compare blowout rates with exact vs heuristic team balancing

**Dependencies**: Slice A (parties) recommended but not required

---

### Slice D: Performance Model & Skill Evolution ‚úÖ **COMPLETE**

**Whitepaper References**: ¬ß2.4 (skill), ¬ß3.7 (performance, skill update), ¬ß6.4 (skill evolution)

**Status**: ‚úÖ **Completed**

**Goals**:
- ‚úÖ Add per-match performance model (KPM/SPM or performance index)
- ‚úÖ Implement skill update rule based on performance vs expectation
- ‚úÖ Track skill distribution evolution over time

**Engine Work**:
- ‚úÖ **`src/types.rs`**:
  - Added to `Player`: `recent_performance: Vec<f64>` (performance indices from recent matches)
  - Added to `Match`: `player_performances: HashMap<usize, f64>` (performance index per player)
  - Added to `MatchmakingConfig`: `skill_learning_rate: f64` (Œ± in update rule, default 0.01), `performance_noise_std: f64` (default 0.15), `enable_skill_evolution: bool` (default true), `skill_update_batch_size: usize` (default 10)
  - Added to `SimulationStats`: `skill_distribution_over_time`, `skill_evolution_enabled`, `total_skill_updates`, `performance_samples`
- ‚úÖ **`src/simulation.rs`**:
  - Implemented `generate_performance()`: generates performance index with base performance based on skill and lobby context, plus configurable noise
  - Implemented `compute_expected_performance()`: computes expected performance (deterministic part) for skill updates
  - Modified `process_match_completions()`: computes performance for each player, updates skills using formula \(s_i^+ = s_i^- + \alpha(\hat{y}_i - \mathbb{E}[Y_i])\), tracks performance samples
  - Added batch update logic: calls `update_skill_percentiles()` every N matches (configurable batch size)
  - Implemented `record_skill_distribution_snapshot()`: records time series of mean skill per bucket
  - Added `matches_since_percentile_update` tracking field to `Simulation`
- ‚úÖ **`src/lib.rs`**:
  - Added `get_skill_evolution_data()` WASM method
  - Added `get_performance_distribution()` WASM method
  - Added `toggle_skill_evolution()` WASM method

**Frontend Work**:
- ‚úÖ Added config controls: `skillLearningRate`, `performanceNoiseStd`, `enableSkillEvolution`, `skillUpdateBatchSize` sliders/checkbox
- ‚úÖ Added skill evolution metrics section in Overview tab (total updates, update rate, evolution mode)
- ‚úÖ Added comprehensive skill evolution visualizations in Distributions tab:
  - Skill evolution over time (line chart showing key buckets: Low 1-2, Mid 5-6, High 9-10)
  - Current skill distribution by bucket (bar chart)
  - Performance distribution histogram
  - Skill drift summary (avg change, most improved/declined buckets)
- ‚úÖ Added toggle button for static vs evolving skill mode
- ‚úÖ Updated stats parsing to include skill evolution metrics

**Metrics & Experiments**:
- ‚úÖ Track: skill drift over time, performance distribution, skill update rate, total skill updates
- ‚úÖ Experiment ready: Compare blowout rates and search times with static vs evolving skill

**Enhancements Beyond Original Plan**:
- Added skill drift summary metrics showing overall skill change and most improved/declined buckets
- Color-coded bucket visualization (red for low, green for high)
- Real-time skill evolution tracking with automatic snapshot recording

**Dependencies**: Slice C (team balancing) - completed, provides accurate performance context

---

### Slice E: Satisfaction, Continuation, and Retention Modeling ‚úÖ **COMPLETE**

**Whitepaper References**: ¬ß3.8 (satisfaction, quit probability), ¬ß6.9 (KPIs)

**Status**: ‚úÖ **Completed** (including return probability)

**Goals**:
- ‚úÖ Replace ad-hoc continuation logic with formal logistic model
- ‚úÖ Define experience vector and parameterized retention function
- ‚úÖ Track per-bucket retention metrics
- ‚úÖ Implement return probability model (between-sessions)
- ‚úÖ Track effective population size and churn rate

**Engine Work**:
- **`src/types.rs`**:
  - Add struct `RetentionConfig`:
    ```rust
    pub struct RetentionConfig {
        pub theta_ping: f64,      // Coefficient for delta ping
        pub theta_search_time: f64,
        pub theta_blowout: f64,
        pub theta_win_rate: f64,
        pub theta_performance: f64,
        pub base_continue_prob: f64,  // Base probability (before penalties)
    }
    ```
  - Add to `Player`: `recent_experience: Vec<ExperienceVector>` (last N matches)
  - Add struct `ExperienceVector`:
    ```rust
    pub struct ExperienceVector {
        pub avg_delta_ping: f64,
        pub avg_search_time: f64,
        pub blowout_rate: f64,
        pub win_rate: f64,
        pub avg_performance: f64,
    }
    ```
- **`src/simulation.rs`**:
  - Add function `compute_continue_probability(player: &Player, config: &RetentionConfig) -> f64`:
    - Build experience vector from recent history
    - Compute: \(P(\text{continue}) = \sigma(\theta^T \mathbf{z}_i)\)
    - Return probability
  - Replace inline continuation logic in `process_match_completions()` with call to `compute_continue_probability()`
  - After each match, update `player.recent_experience`
  - Add to `SimulationStats`: `per_bucket_continue_rate: HashMap<usize, f64>`, `avg_matches_per_session: f64`, `session_length_distribution: Vec<usize>`

**Frontend Work**:
- Add config panel for retention model coefficients
- Add presets: "Ping-First", "Skill-First", "Lenient", "Strict"
- Add chart: continuation rate by skill bucket
- Add chart: average matches per session over time

**Metrics & Experiments**:
- ‚úÖ Track: continuation rate by bucket, matches per session, effective population size (concurrent players)
- ‚úÖ Track: return rate by bucket, churn rate, effective population size over time
- ‚úÖ Track: population change rate (first derivative of effective population, players per second)
- ‚úÖ Diagnostic: average computed continue probability, logit values, experience values, and active config
- ‚úÖ Experiment: Compare population health (total concurrent players, population change rate) with different retention models (Experiment 3 ready)

**Return Probability Implementation**:
- ‚úÖ Added `compute_return_probability()` using same logistic model as continuation
- ‚úÖ Modified `process_arrivals()` to use return probability (threshold-based selection)
- ‚úÖ Preserve last session experience when players quit (goes to `last_session_experience`)
- ‚úÖ Track churn rate (players offline > threshold without returning)
- ‚úÖ Track effective population size over time (sampled every 10 ticks)
- ‚úÖ Track return rate by skill bucket
- ‚úÖ Track population change rate (rate of change of effective population, players per second)
- ‚úÖ Frontend charts: Effective Population Size Over Time, Population Change Rate metric, Return Rate by Skill Bucket
- ‚úÖ Diagnostic panel: Shows average computed continue probability, logit values, experience values, and active retention config for debugging

**Dependencies**: Slice D (performance model) ‚úÖ - completed, provides complete experience vector

---

### Slice F: Region/DC Graph & Regional Metrics ‚úÖ **COMPLETE**

**Whitepaper References**: ¬ß2.3 (DC connectivity), ¬ß2.6 (DCs), ¬ß4 (regions), ¬ß6.1 (regional behavior)

**Status**: ‚úÖ **Completed**

**Goals**:
- ‚úÖ Make regions explicit (enum instead of strings)
- ‚úÖ Define region adjacency graph
- ‚úÖ Add region-aware backoff and tuning
- ‚úÖ Track region-split metrics

**Engine Work**:
- ‚úÖ **`src/types.rs`**:
  - Added enum `Region { NorthAmerica, Europe, AsiaPacific, SouthAmerica, Other }` with `Serialize`, `Deserialize`, `Clone`, `Copy`, `Debug`, `PartialEq`, `Eq`, `Hash`
  - Updated `DataCenter`: `region: Region` (replaced `String`)
  - Updated `Player`: added `region: Region` field
  - Added struct `RegionConfig` with optional overrides:
    ```rust
    pub struct RegionConfig {
        pub max_ping: Option<f64>,
        pub delta_ping_initial: Option<f64>,
        pub delta_ping_rate: Option<f64>,
        pub skill_similarity_initial: Option<f64>,
        pub skill_similarity_rate: Option<f64>,
    }
    ```
  - Added to `MatchmakingConfig`: `region_configs: HashMap<Region, RegionConfig>` with helper methods for region-specific config retrieval
  - Implemented `Region::adjacent_regions() -> Vec<Region>` defining adjacency graph:
    - NA ‚Üî EU (transatlantic), NA ‚Üî SA (Americas)
    - EU ‚Üî APAC (via Middle East/Asia), APAC ‚Üî SA (Pacific)
    - Other is adjacent to all (catch-all)
  - Added struct `RegionStats`:
    ```rust
    pub struct RegionStats {
        pub player_count: usize,
        pub avg_search_time: f64,
        pub avg_delta_ping: f64,
        pub blowout_rate: f64,
        pub active_matches: usize,
        pub cross_region_match_rate: f64,
        pub total_matches: usize,
        pub blowout_count: usize,
        pub cross_region_matches: usize,
    }
    ```
  - Updated `SimulationStats`: added `region_stats: HashMap<Region, RegionStats>` and `cross_region_match_samples: Vec<bool>`
  - Updated `Player::acceptable_dcs()`: implemented region-aware backoff with three-tier system:
    1. Short wait (0-10s): Only best region DCs
    2. Medium wait (10-30s): Best region + adjacent regions
    3. Long wait (30s+): All regions
- ‚úÖ **`src/simulation.rs`**:
  - Updated `init_default_data_centers()`: replaced string regions with `Region` enum variants
  - Updated `generate_population()`: added `determine_region_from_location()` helper using geographic bounds to assign player regions
  - Updated `start_search()`: modified to pass `player.region` and `data_centers` to `acceptable_dcs()`
  - Updated `create_matches()`: added cross-region match detection and tracking
  - Added `update_region_stats()` method: aggregates per-region metrics (search times, delta pings, blowout rates, cross-region match rates)
  - Updated `update_stats()`: calls `update_region_stats()` to populate regional statistics
- ‚úÖ **`src/matchmaker.rs`**:
  - Updated `check_feasibility()`: implemented region-aware DC prioritization (best region ‚Üí adjacent ‚Üí other)
  - Updated `run_tick()`: added cross-region match detection by checking player regions
  - Updated `MatchResult`: added `is_cross_region: bool` field
- ‚úÖ **`src/lib.rs`**:
  - Added `get_region_stats() -> String` WASM method to expose regional statistics
  - Updated `get_data_centers()`: ensured Region enum serializes correctly as string

**Frontend Work**:
- ‚úÖ Added region filter dropdown with "All Regions" option
- ‚úÖ Added region-split charts:
  - Search Time by Region (bar chart)
  - Delta Ping by Region (bar chart)
  - Blowout Rate by Region (bar chart)
  - Cross-Region Match Rate (metric card)
  - Active Matches by Region (bar chart)
- ‚úÖ Added region config UI panel (collapsible section in config panel):
  - Per-region overrides for `maxPing`, `deltaPingInitial`, `deltaPingRate`, `skillSimilarityInitial`, `skillSimilarityRate`
  - Handles nested configuration updates correctly
- ‚úÖ Updated stats parsing: handles `region_stats` JSON with Region enum string keys
- ‚ö†Ô∏è DC map visualization deferred (optional enhancement)

**Metrics & Experiments**:
- ‚úÖ Track: search times by region, cross-region match rate, delta ping by region, blowout rate by region
- ‚úÖ Track: active matches per region, player count per region
- ‚úÖ Experiment ready: Compare behavior in low-population vs high-population regions (Experiment 4 ready)

**Enhancements Beyond Original Plan**:
- Added cross-region match rate tracking and visualization
- Implemented flexible per-region config overrides with fallback to global values
- Added geographic bounds-based region assignment from player location
- Enhanced region stats with additional metrics (total matches, blowout count, cross-region matches)

**Dependencies**: Slice B (backoff refinement) ‚úÖ - completed, provides foundation for region-aware backoff

---

### Slice G: Frontend Experiment Runner & Visualizations ‚úÖ **COMPLETE**

**Whitepaper References**: ¬ß7 (experiments), ¬ß6.9 (KPIs)

**Status**: ‚úÖ **Completed**

**Goals**:
- ‚úÖ Enhance frontend to support all new metrics from slices A-F
- ‚úÖ Build reusable experiment runner UI
- ‚úÖ Add scenario preset system

**Engine Work**:
- ‚úÖ **`src/lib.rs`**:
  - ‚úÖ Verified all new stats/metrics are exposed via WASM (region stats, retention metrics, skill evolution, etc.)
  - ‚úÖ Functions confirmed: `get_region_stats() -> String`, `get_retention_stats() -> String`, `get_skill_evolution_data() -> String`
  - ‚úÖ All WASM bindings verified for complete metric access

**Frontend Work**:
- ‚úÖ **`web/src/utils/ExperimentStorage.js`**:
  - ‚úÖ Comprehensive storage system with localStorage persistence
  - ‚úÖ Export/import JSON functionality
  - ‚úÖ Search, filtering, and tag management
  - ‚úÖ Storage size limits and quota management
  
- ‚úÖ **`web/src/utils/ScenarioPresets.js`**:
  - ‚úÖ Built-in presets for SBMM (Tight, Loose, Skill-First, Ping-First)
  - ‚úÖ Built-in retention presets (Ping-First, Skill-First, Lenient, Strict)
  - ‚úÖ Built-in regional presets (Low Population, High Population)
  - ‚úÖ Built-in party presets (Solo Only, Party Heavy)
  - ‚úÖ Built-in evolution presets (Static Skill, Evolving Skill, High Learning Rate)
  - ‚úÖ Custom preset creation and management
  
- ‚úÖ **`web/src/components/Experiments/ExperimentRunner.jsx`**:
  - ‚úÖ Enhanced experiment runner with single-parameter sweeps
  - ‚úÖ Multi-parameter sweeps (grid search over multiple parameters)
  - ‚úÖ Preset-based experiments
  - ‚úÖ Real-time progress tracking with non-blocking execution
  - ‚úÖ Comprehensive metric collection from all slices
  - ‚úÖ Experiment configuration builder
  
- ‚úÖ **`web/src/components/Experiments/ExperimentLibrary.jsx`**:
  - ‚úÖ Experiment library with grid/list view
  - ‚úÖ Search and filtering by name, tags, type, status, date
  - ‚úÖ Tag management and organization
  - ‚úÖ Batch operations (delete, export)
  - ‚úÖ Experiment details view
  
- ‚úÖ **`web/src/components/Experiments/ExperimentComparison.jsx`**:
  - ‚úÖ Side-by-side comparison (2-4 experiments)
  - ‚úÖ Config difference visualization
  - ‚úÖ Metric overlays on charts
  - ‚úÖ Statistical summaries
  
- ‚úÖ **`web/src/components/Charts/`**:
  - ‚úÖ Reusable MetricChart component
  - ‚úÖ ComparisonChart for overlay comparisons
  - ‚úÖ HeatmapChart for multi-parameter sweep results
  
- ‚úÖ **`web/src/MatchmakingSimulator.jsx`**:
  - ‚úÖ New "Experiments" tab with enhanced runner
  - ‚úÖ New "Experiment Library" tab for management
  - ‚úÖ New "Comparison" tab for side-by-side analysis
  - ‚úÖ Integration of all experiment components
  
- ‚úÖ **`web/src/hooks/useExperimentRunner.js`**:
  - ‚úÖ Optimized experiment execution with batched tick processing
  - ‚úÖ Non-blocking execution using requestAnimationFrame
  - ‚úÖ Progress updates that don't freeze the UI

**Metrics & Experiments**:
- ‚úÖ All experiments from slices A-F are runnable from UI
- ‚úÖ Experiment storage and management system enables research-grade workflows
- ‚úÖ Export/import functionality for sharing and archival
- ‚úÖ All canonical experiments can be run via the new experiment runner

**Enhancements Beyond Original Plan**:
- ‚úÖ Full experiment management system (CRUD operations, search, filtering)
- ‚úÖ Experiment comparison tool for side-by-side analysis
- ‚úÖ Non-blocking execution optimization: Batched tick processing (50-100 ticks per batch) with requestAnimationFrame yielding to prevent UI freezing during long experiments
- ‚úÖ Comprehensive metric collection including all slices A-F metrics
- ‚úÖ Tag-based organization system
- ‚úÖ Bulk export/import capabilities
- ‚úÖ Experiment builder UI for visual configuration
- ‚úÖ Duration estimation for experiment planning
- ‚úÖ Real-time progress tracking with smooth UI updates

**Dependencies**: Slices A-F ‚úÖ - all metrics implemented and verified

---

### Slice H (Optional): Aggregate / Reduced Model

**Whitepaper References**: ¬ß5 (aggregate model), ¬ß7 Stage 4

**Goals**:
- Implement bucketed/ODE-style model for massive scale
- Derive pairing kernel and throughput functions from micro-sim
- Validate aggregate model against agent-based model

**Engine Work**:
- **`src/aggregate.rs`** (new module):
  - Define bucket structure: \((r, m, b, k)\) where \(r\)=region, \(m\)=playlist, \(b\)=skill bucket, \(k\)=wait bin
  - State variables: \(S_{rmbk}(t)\), \(P_{rmb}(t)\), \(H_{rmb}(t)\)
  - Implement ODE update rules:
    - Arrivals: \(\lambda_{rmb}(t)\)
    - Aging between wait bins
    - Match throughput: \(\mu_{rmbk}(t)\)
    - Match completions: \(P_{rmb}(t) / \mathbb{E}[L_m]\)
  - Implement pairing kernel \(K_{bb'}\) (empirically fit from micro-sim or analytical approximation)
  - Implement throughput function: \(\nu_{rm}(t) = \min(S_{rm}(t) / N_m^{\text{req}}, \sum_d F_{d,m}(t))\)
- **`src/simulation.rs`**:
  - Add function `export_micro_data() -> AggregateTrainingData` (export samples for fitting)
- **`src/lib.rs`**:
  - Add `AggregateSimulation` struct and WASM bindings
  - Add function `run_aggregate_simulation(config, initial_state) -> AggregateResults`

**Frontend Work**:
- Add toggle: "Micro" vs "Aggregate" simulation mode
- Run same experiments in both modes and compare results
- Visualize pairing kernel \(K_{bb'}\) as heatmap

**Metrics & Experiments**:
- Validate: aggregate model reproduces micro-model outputs (search times, delta ping, blowouts) within acceptable error
- Experiment: Run long-term scenarios (months) with aggregate model

**Dependencies**: Slices A-F ‚úÖ (complete micro-model available for parameter fitting)

---

## Implementation Phases

Phases group slices into logical execution order. Each phase produces working artifacts and can be validated independently.

### Phase 1: Core Matchmaking Fidelity

**Slices**: A (Parties) ‚úÖ **COMPLETE** + B (Constraints/Backoff) ‚úÖ **COMPLETE**

**Goal**: Complete the core matchmaking loop with parties and accurate constraints.

**Deliverables**:
- ‚úÖ Parties fully integrated into search and matchmaking
- ‚úÖ Backoff functions match whitepaper formulas exactly (verified with unit tests)
- ‚ö†Ô∏è Under-full lobby support (optional, deferred)
- ‚úÖ Debug logging for feasibility failures (behind feature flag)

**Validation**:
- ‚úÖ Run simulation with parties and verify: party integrity maintained, search times reasonable
- ‚úÖ Verify backoff curves match expected formulas (unit tests added)
- ‚úÖ Verify skill range constraints work correctly (unit tests added)
- ‚ö†Ô∏è Compare search times with/without under-full lobbies in low-population scenarios (deferred)

**Status**: Phase 1 complete. Both slices A and B implemented and tested.

**Estimated Effort**: 2-3 weeks (Slice A: ~1 week, Slice B: ~1 week)

---

### Phase 2: Match Quality & Outcomes

**Slices**: C (Team Balancing/Blowouts) ‚úÖ **COMPLETE** + D (Performance/Skill Evolution) ‚úÖ **COMPLETE**

**Goal**: Improve match quality prediction and enable dynamic skill evolution.

**Deliverables**:
- ‚úÖ Exact team balancing for small playlists
- ‚úÖ Enhanced blowout detection with severity
- ‚úÖ Performance model and skill update rule
- ‚úÖ Skill distribution evolution tracking

**Validation**:
- ‚úÖ Compare blowout rates with exact vs heuristic balancing (ready for testing)
- ‚úÖ Verify skill evolution: players improve/decline based on performance (implemented and ready for testing)
- ‚úÖ Track skill distribution stability over long runs (tracking implemented)

**Status**: Phase 2 complete. Both slices C and D implemented and ready for validation.

**Estimated Effort**: 3-4 weeks (Slice C: ~1 week, Slice D: ~2-3 weeks) - **COMPLETED**

---

### Phase 3: Player Behavior & Regional Analysis

**Slices**: E (Retention) ‚úÖ **COMPLETE** + F (Regions) ‚úÖ **COMPLETE**

**Goal**: Model player satisfaction and enable regional analysis.

**Deliverables**:
- ‚úÖ Formal retention model with experience vector
- ‚úÖ Return probability model (between-sessions)
- ‚úÖ Effective population size and churn tracking
- ‚úÖ Region adjacency graph and region-aware backoff
- ‚úÖ Per-region metrics and analysis
- ‚úÖ Retention presets (ping-first, skill-first, lenient, strict)
- ‚úÖ Population change rate metric (tracks rate of change of effective population)
- ‚úÖ Diagnostic panel for retention model debugging
- ‚úÖ Region-split charts and region filter controls
- ‚úÖ Per-region configuration overrides

**Validation**:
- ‚úÖ Compare population health (concurrent players, population change rate) with different retention models (Experiment 3 ready)
- ‚úÖ Analyze regional differences: search times, delta ping, blowout rates (Slice F complete)
- ‚úÖ Verify low-population regions can spill into adjacent regions (region-aware backoff implemented)

**Status**: Phase 3 complete. Both slices E and F implemented and ready for validation.

**Estimated Effort**: 2-3 weeks (Slice E: ‚úÖ Complete, Slice F: ‚úÖ Complete)

---

### Phase 4: Frontend & Experimentation ‚úÖ **COMPLETE**

**Slice**: G (Frontend Enhancements) ‚úÖ **COMPLETE**

**Goal**: Make all new features accessible via UI and enable comprehensive experiments.

**Deliverables**:
- ‚úÖ All new metrics visualized (retention, skill evolution, regions, blowouts)
- ‚úÖ Enhanced experiment runner (multi-param sweeps, config comparison)
- ‚úÖ Scenario preset system with built-in presets for all major categories
- ‚úÖ Region filters and regional analysis tools
- ‚úÖ Comprehensive experiment management system
- ‚úÖ Export/import functionality for experiments
- ‚úÖ Non-blocking experiment execution (optimized to prevent UI freezing)

**Validation**:
- ‚úÖ Run all canonical experiments from roadmap via UI (Experiments 1-6 ready)
- ‚úÖ Scenario presets implemented and available for use
- ‚úÖ Experiment runner tested with various parameter combinations
- ‚úÖ Storage, search, filtering, and comparison tools functional

**Status**: Phase 4 complete. All deliverables implemented and integrated.

**Estimated Effort**: 2-3 weeks - **COMPLETED**

---

### Phase 5 (Optional): Aggregate Model

**Slice**: H (Aggregate/ODE Model)

**Goal**: Enable massive-scale simulations via reduced model.

**Deliverables**:
- Bucketed ODE model implementation
- Pairing kernel and throughput functions (fitted from micro-sim)
- Aggregate simulation driver
- Validation against micro-model

**Validation**:
- Run identical scenarios in micro and aggregate modes
- Compare outputs: search times, delta ping, blowouts, retention
- Verify aggregate model runs 100x+ faster for large populations

**Estimated Effort**: 4-6 weeks

---

## Experiment Catalog

This section documents canonical experiments that can be run once the relevant slices are implemented. Each experiment should be reproducible via the frontend experiment runner.

### Experiment 1: SBMM Strictness Sweep ‚úÖ **READY**

**Dependencies**: Slices A ‚úÖ, B ‚úÖ, C ‚úÖ

**Parameters**: Vary `skill_similarity_initial` from 0.01 to 0.3

**Metrics to Track**:
- Search time (P50, P90, P99) by skill bucket
- Delta ping by skill bucket
- Blowout rate overall and by bucket (now includes severity breakdown)
- Skill disparity distribution
- Team skill difference distribution (new from Slice C)
- Per-playlist blowout rates (new from Slice C)

**Expected Results**:
- Tighter SBMM ‚Üí longer search times, especially for extreme skill buckets
- Tighter SBMM ‚Üí lower blowout rate, better skill matching
- Tradeoff: search time vs match quality
- Exact team balancing should reduce blowout rates compared to snake draft

**Config Preset**: `experiments/sbmm_strictness_sweep.json`

**Status**: All dependencies complete. Experiment can be run with enhanced metrics from Slice C.

---

### Experiment 2: Ping vs Skill Weight Tradeoff

**Dependencies**: Slices A, B

**Parameters**: Vary `weight_skill` from 0.1 to 0.7 (with `weight_geo` = 1.0 - `weight_skill`)

**Metrics to Track**:
- Average delta ping
- Average search time
- Skill disparity
- Blowout rate

**Expected Results**:
- Higher skill weight ‚Üí better skill matching, worse ping
- Higher geo weight ‚Üí better ping, worse skill matching
- Optimal point depends on population density

**Config Preset**: `experiments/ping_vs_skill_tradeoff.json`

---

### Experiment 3: Retention Model Comparison ‚úÖ **READY**

**Dependencies**: Slices E ‚úÖ, D ‚úÖ

**Parameters**: Compare retention presets: "Ping-First", "Skill-First", "Lenient", "Strict"

**Metrics to Track**:
- ‚úÖ Effective population size (concurrent players) over time
- ‚úÖ Population change rate (players per second, positive = growing, negative = shrinking)
- ‚úÖ Average matches per session
- ‚úÖ Continuation rate by skill bucket
- ‚úÖ Churn rate
- ‚úÖ Return rate by skill bucket
- ‚úÖ Diagnostic metrics: average computed continue probability, logit values, experience values

**Expected Results**:
- Ping-First ‚Üí higher retention for low-ping players, lower for high-ping
- Skill-First ‚Üí higher retention for mid-skill players
- Lenient ‚Üí higher overall retention but more blowouts
- Strict ‚Üí lower retention but better match quality

**Config Preset**: `experiments/retention_model_comparison.json`

**Status**: All dependencies complete. Experiment can be run with full return probability and population health tracking.

---

### Experiment 4: Regional Population Effects ‚úÖ **READY**

**Dependencies**: Slices F ‚úÖ, B ‚úÖ

**Parameters**: Vary regional population weights (e.g., NA: 0.7, EU: 0.2, APAC: 0.1 vs balanced 0.33 each)

**Metrics to Track**:
- ‚úÖ Search time by region
- ‚úÖ Delta ping by region
- ‚úÖ Cross-region match rate
- ‚úÖ Blowout rate by region
- ‚úÖ Active matches by region
- ‚úÖ Player count by region

**Expected Results**:
- Low-population regions ‚Üí longer search times, higher delta ping (spill to other regions)
- High-population regions ‚Üí shorter search times, better ping
- Regional backoff helps but doesn't eliminate disparities
- Region-aware backoff should show three-tier expansion (best ‚Üí adjacent ‚Üí all) as wait time increases

**Config Preset**: `experiments/regional_population_effects.json`

**Status**: All dependencies complete. Experiment can be run with full regional metrics tracking and region-aware backoff.

---

### Experiment 5: Skill Evolution Over Time ‚úÖ **READY**

**Dependencies**: Slices D ‚úÖ, C ‚úÖ

**Parameters**: Compare "Static Skill" vs "Evolving Skill" modes over long runs (1000+ ticks)

**Metrics to Track**:
- Skill distribution evolution (mean, variance by bucket)
- Blowout rate over time
- Search time trends
- Performance distribution by skill bucket
- Skill drift metrics (avg change, most improved/declined buckets)

**Expected Results**:
- Evolving skill ‚Üí skill distribution may shift (e.g., players improve)
- Evolving skill ‚Üí blowout rates may change as skill estimates improve
- Static skill ‚Üí stable but potentially unrealistic

**Config Preset**: `experiments/skill_evolution_comparison.json`

**Status**: All dependencies complete. Experiment can be run with full skill evolution tracking and visualizations.

---

### Experiment 6: Party Size Effects

**Dependencies**: Slice A

**Parameters**: Vary party size distribution (solo-only vs 50% parties of size 2-4)

**Metrics to Track**:
- Search time for solo vs party players
- Match rate by party size
- Skill disparity within parties
- Team balance quality (with parties)

**Expected Results**:
- Larger parties ‚Üí longer search times (harder to find compatible matches)
- Parties maintain skill cohesion better than random groups
- Team balancing with parties is more constrained

**Config Preset**: `experiments/party_size_effects.json`

---

## Next Steps

1. **Review this roadmap** and confirm slice priorities
2. **Start with Phase 1** (Slices A + B) for core matchmaking fidelity
3. **Implement incrementally**: Complete one slice, validate, then move to next
4. **Update roadmap** as you discover gaps or adjust scope
5. **Document findings**: Add "Results" sections to slices as you complete them

---

## Notes

- **Intentional Simplifications**: Some whitepaper features are intentionally simplified or deferred:
  - Map diversity/rotation (low priority)
  - Voice chat matching (weak signal, can ignore)
  - Platform-specific optimizations (can treat as cross-platform penalty only)
- **Performance**: Current implementation handles ~5k-10k players comfortably. For larger populations, use aggregate model (Slice H).
- **Testing**: Each slice should include unit tests and integration tests. Use property-based tests where possible (e.g., party integrity, backoff monotonicity).

---

**Version**: 1.0

